{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8fd260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import useful libraries.\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "from scipy import stats\n",
    "import math\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import matplotlib\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial import distance\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from scipy.stats import mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177ed8d3",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdaaa14",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8ae249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load directory of data.\n",
    "direct = ''\n",
    "mat = scipy.io.loadmat(direct) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0bc13d",
   "metadata": {},
   "source": [
    "### Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58fc09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset (allspec) into training set and test set.\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_all, # define the spectrum data\n",
    "                                                    np.ravel(label_number), # label of spectrum data drug MoAs\n",
    "                                                    test_size = 0.3, \n",
    "                                                    stratify = np.ravel(label_number), \n",
    "                                                    random_state=109) # 70% training and 30% test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1106fab4",
   "metadata": {},
   "source": [
    "## Model construction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65743db3",
   "metadata": {},
   "source": [
    "### LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395460db",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_clf = LinearDiscriminantAnalysis(priors = prior, \n",
    "                                     store_covariance = True).fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ae71cc",
   "metadata": {},
   "source": [
    "### Model evalutaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b2786d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model performance using accuracy.\n",
    "lda_pred = lda_clf.predict(X_test)\n",
    "lda_pred_accu = accuracy_score(y_test, lda_pred)\n",
    "print(\"Accuracy lda for allspec:\", lda_pred_accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05649e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix.\n",
    "cm = confusion_matrix(y_test, lda_pred, normalize='true')\n",
    "# label of each drug MoA.\n",
    "name_list = ['Control','PSI','PDI','DNAI/TopoII','LFACS','FASN','mTOR/PI3K','EGFR/HER2','EGFR','PARP']\n",
    "\n",
    "df_cm = pd.DataFrame(cm, index = [i for i in name_list],\n",
    "                     columns = [i for i in name_list])\n",
    "plt.figure(figsize = (10,6))\n",
    "plt.rcParams.update({'font.size':10})\n",
    "plt.title('Confusion matrix of LDA classifier')\n",
    "sns.heatmap(df_cm, annot=True, fmt='.2%')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9878b8e",
   "metadata": {},
   "source": [
    "## Novelty detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2420fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA reduction to keep top 50 principal components in novelty detection, \n",
    "# we found this step will generate more reasonable visualization results in LDA plots when there is new data from test compound,\n",
    "# as the total of 288 dimmensions is hard to be kept in only 3 dimmensions. Otherwise, the distribution of test compound will\n",
    "# be very dispersed. This step also helps in predict the closest group.\n",
    "pca = PCA(n_components=50,svd_solver='full')\n",
    "data_mean = np.mean(data_all,axis = 0)\n",
    "data_all_reduced = pca.fit_transform(data_all - data_mean) # centralize data before PCA.\n",
    "\n",
    "newspec = cellspec_epi # Use epirubicin spec as an example of the tested compound.\n",
    "newspec_reduced = pca.transform(newspec - data_mean) # Transform spec of tested compound to PCA reduction space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ae2365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For novelty detection, we can construct a new LDA model using all the data.\n",
    "lda_clf_novel = LinearDiscriminantAnalysis(priors = prior, \n",
    "                                           store_covariance = True).fit(data_all_reduced,np.ravel(label_number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7419b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple way to calcualte the closest reference group is to directly use the predict function in LDA, \n",
    "# which intrinsically calculated the Mahalanobis distance.\n",
    "idx_matrix = lda_clf_novel.predict(newspec_reduced)\n",
    "idx = mode(idx_matrix)[0]\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "2c707562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To obtain the specific/quantative Mahalanobis distance, reference the source code at:\n",
    "# https://github.com/scikit-learn/scikit-learn/blob/2d8e03f4d/sklearn/discriminant_analysis.py#L177\n",
    "def empirical_covariance(X, *, assume_centered=False):\n",
    "    \"\"\"Compute the Maximum likelihood covariance estimator.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray of shape (n_samples, n_features)\n",
    "        Data from which to compute the covariance estimate.\n",
    "    assume_centered : bool, default=False\n",
    "        If `True`, data will not be centered before computation.\n",
    "        Useful when working with data whose mean is almost, but not exactly\n",
    "        zero.\n",
    "        If `False`, data will be centered before computation.\n",
    "    Returns\n",
    "    -------\n",
    "    covariance : ndarray of shape (n_features, n_features)\n",
    "        Empirical covariance (Maximum Likelihood Estimator).\n",
    "    \"\"\"\n",
    "    X = np.asarray(X)\n",
    "\n",
    "    if X.ndim == 1:\n",
    "        X = np.reshape(X, (1, -1))\n",
    "\n",
    "    if X.shape[0] == 1:\n",
    "        warnings.warn(\n",
    "            \"Only one sample available. You may want to reshape your data array\"\n",
    "        )\n",
    "\n",
    "    if assume_centered:\n",
    "        covariance = np.dot(X.T, X) / X.shape[0]\n",
    "    else:\n",
    "        covariance = np.cov(X.T, bias=1)\n",
    "\n",
    "    if covariance.ndim == 0:\n",
    "        covariance = np.array([[covariance]])\n",
    "    return covariance\n",
    "\n",
    "\n",
    "def _cov(X, shrinkage=None, covariance_estimator=None):\n",
    "    \"\"\"Estimate covariance matrix (using optional covariance_estimator).\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like of shape (n_samples, n_features)\n",
    "        Input data.\n",
    "    shrinkage : {'empirical', 'auto'} or float, default=None\n",
    "        Shrinkage parameter, possible values:\n",
    "          - None or 'empirical': no shrinkage (default).\n",
    "          - 'auto': automatic shrinkage using the Ledoit-Wolf lemma.\n",
    "          - float between 0 and 1: fixed shrinkage parameter.\n",
    "        Shrinkage parameter is ignored if  `covariance_estimator`\n",
    "        is not None.\n",
    "    covariance_estimator : estimator, default=None\n",
    "        If not None, `covariance_estimator` is used to estimate\n",
    "        the covariance matrices instead of relying on the empirical\n",
    "        covariance estimator (with potential shrinkage).\n",
    "        The object should have a fit method and a ``covariance_`` attribute\n",
    "        like the estimators in :mod:`sklearn.covariance``.\n",
    "        if None the shrinkage parameter drives the estimate.\n",
    "        .. versionadded:: 0.24\n",
    "    Returns\n",
    "    -------\n",
    "    s : ndarray of shape (n_features, n_features)\n",
    "        Estimated covariance matrix.\n",
    "    \"\"\"\n",
    "    if covariance_estimator is None:\n",
    "        shrinkage = \"empirical\" if shrinkage is None else shrinkage\n",
    "        if isinstance(shrinkage, str):\n",
    "            if shrinkage == \"auto\":\n",
    "                sc = StandardScaler()  # standardize features\n",
    "                X = sc.fit_transform(X)\n",
    "                s = ledoit_wolf(X)[0]\n",
    "                # rescale\n",
    "                s = sc.scale_[:, np.newaxis] * s * sc.scale_[np.newaxis, :]\n",
    "            elif shrinkage == \"empirical\":\n",
    "                s = empirical_covariance(X)\n",
    "        elif isinstance(shrinkage, Real):\n",
    "            s = shrunk_covariance(empirical_covariance(X), shrinkage)\n",
    "    else:\n",
    "        if shrinkage is not None and shrinkage != 0:\n",
    "            raise ValueError(\n",
    "                \"covariance_estimator and shrinkage parameters \"\n",
    "                \"are not None. Only one of the two can be set.\"\n",
    "            )\n",
    "        covariance_estimator.fit(X)\n",
    "        if not hasattr(covariance_estimator, \"covariance_\"):\n",
    "            raise ValueError(\n",
    "                \"%s does not have a covariance_ attribute\"\n",
    "                % covariance_estimator.__class__.__name__\n",
    "            )\n",
    "        s = covariance_estimator.covariance_\n",
    "    return s\n",
    "\n",
    "def _class_cov(X, y, shrinkage=None, covariance_estimator=None):\n",
    "    \"\"\"Compute weighted within-class covariance matrix.\n",
    "    The per-class covariance are weighted by the class priors.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like of shape (n_samples, n_features)\n",
    "        Input data.\n",
    "    y : array-like of shape (n_samples,) or (n_samples, n_targets)\n",
    "        Target values.\n",
    "    priors : array-like of shape (n_classes,)\n",
    "        Class priors.\n",
    "    shrinkage : 'auto' or float, default=None\n",
    "        Shrinkage parameter, possible values:\n",
    "          - None: no shrinkage (default).\n",
    "          - 'auto': automatic shrinkage using the Ledoit-Wolf lemma.\n",
    "          - float between 0 and 1: fixed shrinkage parameter.\n",
    "        Shrinkage parameter is ignored if `covariance_estimator` is not None.\n",
    "    covariance_estimator : estimator, default=None\n",
    "        If not None, `covariance_estimator` is used to estimate\n",
    "        the covariance matrices instead of relying the empirical\n",
    "        covariance estimator (with potential shrinkage).\n",
    "        The object should have a fit method and a ``covariance_`` attribute\n",
    "        like the estimators in sklearn.covariance.\n",
    "        If None, the shrinkage parameter drives the estimate.\n",
    "        .. versionadded:: 0.24\n",
    "    Returns\n",
    "    -------\n",
    "    cov : array-like of shape (n_features, n_features)\n",
    "        Weighted within-class covariance matrix\n",
    "    \"\"\"\n",
    "    classes = np.unique(y)\n",
    "    cov = np.zeros(shape=(X.shape[1], X.shape[1]))\n",
    "    priors = np.bincount(y) / float(len(y))\n",
    "    for idx, group in enumerate(classes):\n",
    "        Xg = X[y == group]\n",
    "        cov += priors[idx] * np.atleast_2d(_cov(Xg, shrinkage, covariance_estimator))\n",
    "    return cov\n",
    "\n",
    "def get_class(X, y, cov_X, number):\n",
    "    \"\"\"\n",
    "    Compute the mean of each class and the inverse of covariance of each class\n",
    "    Parameters\n",
    "    -------\n",
    "    X: data\n",
    "    Y: label\n",
    "    cov_X: covariance of X, can be computed by _class_cov function\n",
    "    number: number of classes\n",
    "    Returns\n",
    "    -------\n",
    "    class_mean_list : centroid of each class\n",
    "    covinv_list: inverse of covariance of each class\n",
    "    \"\"\"\n",
    "    class_mean_list = []\n",
    "    covinv_list = []\n",
    "    for i in range(number):\n",
    "        cl = X[y == i]\n",
    "        cov = cov_X\n",
    "        inverse=np.linalg.pinv(cov)\n",
    "        cl_mean = np.mean(cl,axis=0)\n",
    "        class_list.append(cl)\n",
    "        class_mean_list.append(cl_mean)\n",
    "        covinv_list.append(inverse)\n",
    "    return class_mean_list, covinv_list\n",
    "\n",
    "def get_mahalanobis_dist(class_mean_list, convinv_list, spec):\n",
    "    \"\"\"\n",
    "    Compute the Mahalanobis distance between the input spec data and each class\n",
    "    Parameters\n",
    "    -----------\n",
    "    class_mean_list: centroid of each class\n",
    "    covinv_list: inverse of covariance of each class\n",
    "    spec: spec of tested compound\n",
    "    Returns\n",
    "    -------\n",
    "    dist_matrix : Mahalanobis distance of the sepc data from tested compound compared with each class centroid\n",
    "    \"\"\"\n",
    "    dist_matrix = np.zeros((len(spec),len(class_mean_list)))\n",
    "    for i in range(len(spec)):\n",
    "        for j in range(len(class_mean_list)):\n",
    "            dist = distance.mahalanobis(spec[i,:], class_mean_list[j], convinv_list[j])\n",
    "            dist_matrix[i,j] = dist\n",
    "    return dist_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750ce766",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_X = _class_cov(data_all_reduced, np.ravel(label_number)) # Compute covariance of each class\n",
    "\n",
    "class_mean_list, covinv_list = get_class(data_all_reduced, \n",
    "                                         np.ravel(label_number), cov_X, 10) # Compute centroid and inverse of covrariance matrix\n",
    "\n",
    "dist_m = get_mahalanobis_dist(class_mean_list,\n",
    "                              covinv_list,\n",
    "                              newspec_reduced) # Compute Mahalanobis distance of tested spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153fbe11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average novelty prediction score using isolation forest \n",
    "# to detect whether the tested compound is outlier compared to its closest reference group.\n",
    "nov_list = []\n",
    "clf = IsolationForest(contamination = 0.2, \n",
    "                      random_state= 10,\n",
    "                      n_estimators = 1000).fit(data_all[np.ravel(label_number) == idx])\n",
    "\n",
    "nov = clf.predict(newspec)\n",
    "print(\"novelty score: \", np.mean(nov))\n",
    "print(\"outlier percentage:\", len(nov[nov == -1])/len(nov))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
